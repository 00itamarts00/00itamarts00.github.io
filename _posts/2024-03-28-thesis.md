---
layout: post
title: Uncovering Strong Lottery Ticket Using Stochastic Gates
subtitle: Thesis
cover-img: assets/img/thesis_cover.jpg
tags: [ML, DL, ModelCompression, EfficientAI, InferenceAcceleration, EdgeComputing, Research]
comments: true
mathjax: true
author: Itamar Tsayag
---

# Uncovering Strong Lottery Tickets with Stochastic Gates

## Authors
**Itamar Tsayag**  
Bar Ilan University  
itamar.tsay@gmail.com  

**Dr. Ofir Lindenbaum**  
Bar Ilan University  

## Abstract
Deep neural networks have achieved remarkable success across various domains, largely due to their over-parameterization, which enhances optimization and generalization capabilities. However, this over-parameterization comes at the cost of increased memory requirements and computational complexity, making deployment on resource-constrained devices challenging.

To address this, model compression techniques such as neural network pruning aim to reduce model size and computational demands while preserving performance. The Lottery Ticket Hypothesis (LTH) suggests that within these over-parameterized networks exist smaller, trainable subnetworks that can achieve comparable performance to the original model.

Existing approaches often rely on iterative pruning or non-differentiable techniques like the pop-up score, limiting efficiency. We propose a novel method leveraging continuously relaxed Bernoulli variables that act as gates to uncover sparse, high-performing subnetworks directly at initialization. Unlike traditional pruning approaches, our method eliminates the need for weight training by optimizing only the gating mechanism, leaving the original weights unmodified.

This enables us to achieve up to 90% sparsity with minimal accuracy loss across diverse architectures, including LeNet, CNNs, and Transformers, making it an efficient and scalable alternative to existing techniques.

## Keywords
Neural network pruning, Lottery Ticket Hypothesis (LTH), Stochastic Gates (STG), Model compression, Transformer pruning, Sparse neural networks

---

## 1. Introduction
The increasing complexity and scale of modern deep learning models have significantly amplified their computational demands. While larger models generally achieve higher accuracy, their substantial resource requirements limit accessibility for many practitioners.

A key factor contributing to these demands is the over-parameterization commonly observed in large neural networks. Over-parameterized models often contain redundant components that do not substantially improve performance but still consume considerable computational resources. To address this inefficiency, neural network compression techniques—such as pruning, quantization, knowledge distillation, and low-rank factorization—have been developed to reduce memory usage and computational costs while maintaining accuracy.

Among these techniques, pruning has gained significant attention as it seeks to identify and remove unnecessary parameters while preserving the network’s predictive capabilities. This aligns closely with the Lottery Ticket Hypothesis (LTH), which presents an intriguing perspective: within large, over-parameterized networks, there exist smaller subnetworks—referred to as "winning tickets"—that can achieve performance comparable to the original model when trained in isolation.

### Placeholder for Figure 1: Illustration of LTH and Strong Lottery Tickets

## 2. Related Work
The Lottery Ticket Hypothesis (LTH), introduced by Frankle and Carbin, posits that within a randomly initialized dense neural network lies a sparse subnetwork—termed a winning ticket—that can be trained in isolation to match the accuracy of the original network. Winning tickets require fewer parameters and training epochs, making them computationally efficient and less prone to overfitting.

Winning tickets are identified through iterative magnitude-based pruning (MBP), where weights with the lowest magnitude are pruned, and the remaining weights are reset to their initial values. This iterative process produces smaller networks that, after training, achieve performance comparable to the original.

Building on LTH, Ramanujan et al. introduced the concept of strong lottery tickets (SLTs), subnetworks that achieve competitive accuracy without any additional training. Using the edge-popup algorithm, they demonstrated that a randomly initialized Wide ResNet-50 contains a subnetwork matching the performance of a trained ResNet-34 on ImageNet.

Despite its success, the edge-popup algorithm has key limitations. Its reliance on a non-differentiable gradient estimator makes optimization inefficient and computationally expensive, restricting its scalability to larger networks. To address these challenges, we propose a fully differentiable stochastic gating mechanism (STG) that enables efficient and direct identification of strong LT networks.

## 3. Proposed Solution
We introduce a novel approach to uncovering SLTs using Stochastic Gates (STG). STG facilitates unstructured pruning (at the weight level) by applying learnable gating variables, which iteratively determine the relevance of each weight. The gating variable for a weight between neurons i and j in layer l is defined as:

```
z_l_ij = max(0, min(1, μ_l_ij + ε_l_ij))
```

where ε_l_ij follows a Gaussian distribution, and μ_l_ij is a learned parameter. A hard-sigmoid function ensures the gating variable remains in the range [0,1].

To induce sparsity, the objective function incorporates a regularization term:

```
min {W(i), B(i)} L({B(i)⊙W(i)h(i−1)}L_i=1) + λ Σ ∥B(i)∥0
```

This method provides a differentiable, efficient pruning strategy, outperforming existing algorithms while supporting unstructured sparsification for flexible network compression.

## 4. Experiments
### STG Sparsification of LeNet-300-100
A LeNet-300-100 base network was used with an STG gating network of the same size. The gating network’s weights serve as both a pruning mask and a fine-tuning mechanism. Three sparsification settings were tested:
- **Post-training pruning**: Trained model weights were frozen, and the gating network pruned unnecessary weights, achieving 30% sparsification.
- **Continuous sparsification and training**: Both networks were trained jointly, achieving 43% sparsification while maintaining accuracy.
- **Pre-training pruning**: The base network was randomly initialized and frozen, with the gating network uncovering an SLT subnetwork achieving 93% accuracy with 48% sparsification.

### Placeholder for Figure 2: Sparsification Process for LeNet-300-100

### STG Sparsification of CNNs
The method was evaluated on CNNs trained on CIFAR10. Using a ResNet50 network initialized with ImageNet weights, we achieved:
- **Top-1 accuracy**: 83.1%
- **Sparsification**: 91.5%

### Placeholder for Figure 3: Sparsification per Layer of ResNet-50

### STG Sparsification of Transformer-Based Networks
The method was extended to Transformer architectures such as ViT-base and Swin Transformer, achieving:
- **ViT-base**: 76% accuracy, 90% sparsification
- **Swin-T**: 80% accuracy, 50% sparsification

### Placeholder for Figure 4: Strong Lottery Tickets in Transformers

## 5. Conclusion and Future Work
This work explored uncovering strong lottery ticket subnetworks using Stochastic Gates (STG). We demonstrated that randomly initialized over-parameterized neural networks contain subnetworks with competitive performance. STG enables high sparsity with minimal accuracy loss, significantly reducing model size and computational demands.

Future research could optimize STG parameters, introduce adaptive mechanisms for dynamic sparsity-accuracy balancing, and extend applications to NLP, reinforcement learning, and time-series forecasting.

## 6. References
1. Frankle, J., & Carbin, M. (2018). The lottery ticket hypothesis: Finding sparse, trainable neural networks.
2. Ramanujan, V., et al. (2020). What’s hidden in a randomly weighted neural network?
3. Yamada, Y., et al. (2020). Feature selection using stochastic gates.

(*Full reference list omitted for brevity, should be included in final document.*)

---