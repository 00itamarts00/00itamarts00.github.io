---
layout: post
title: Uncovering Strong Lottery Ticket Using Stochastic Gates
subtitle: Thesis
cover-img: assets/img/thesis_cover.jpg
tags: [ML, DL, ModelCompression, EfficientAI, InferenceAcceleration, EdgeComputing, Research]
comments: true
mathjax: true
author: Itamar Tsayag
---

Deep learning models have become increasingly powerful, but their growing complexity often leads to inefficiencies in deployment, particularly on edge devices and real-time systems. My MSc thesis at Bar-Ilan University focused on **model compression and optimization**, aiming to reduce the computational burden of neural networks while maintaining high accuracy. This research is particularly relevant for applications in computer vision and speech processing, where inference speed and memory efficiency are critical.

## Problem Statement

Neural networks typically require significant resources for both training and inference. High parameter counts, redundant computations, and inefficient memory utilization lead to slow inference times and increased energy consumption. The challenge is to **compress and optimize these models** while ensuring minimal loss of accuracy.

### Key Questions:
1. How can we **reduce model size** without compromising predictive performance?
2. What are the most **effective pruning and quantization techniques** for deep learning models?
3. Can we develop an efficient **training pipeline** that incorporates optimization techniques without requiring extensive retraining?

## Research Approach

### 1. Model Pruning
Pruning techniques aim to remove unnecessary weights from a network. My research explored **structured pruning** (removing entire neurons or channels) and **unstructured pruning** (removing individual weights with low importance). I analyzed:
- Magnitude-based pruning
- L1 and L2 norm regularization for sparsity
- Lottery Ticket Hypothesis approaches

### 2. Quantization
Quantization reduces the precision of numerical representations in a model, allowing it to run efficiently on hardware with limited computational capacity. I implemented:
- **Post-training quantization (PTQ)** – Applying quantization after training without fine-tuning.
- **Quantization-aware training (QAT)** – Training the model while considering lower precision.

### 3. Knowledge Distillation
In this method, a **large teacher model** transfers knowledge to a smaller **student model**, effectively compressing the network while maintaining performance. I examined:
- Soft target distillation
- Feature map distillation
- Cross-entropy loss distillation

## Experimental Setup

### Datasets Used:
- **Image Classification**: CIFAR-10, ImageNet
- **Speech Processing**: LibriSpeech

### Hardware:
- NVIDIA A100 GPU for large-scale experiments
- Edge devices for real-world performance evaluation

### Evaluation Metrics:
- **Compression Ratio** (Model size reduction)
- **Inference Time** (Milliseconds per inference)
- **Top-1 & Top-5 Accuracy** (Comparison with original model)

## Key Findings

1. **Structured pruning** yielded the best balance between compression and accuracy, reducing model size by **40% with less than 1% accuracy drop**.
2. **Quantization-aware training (QAT)** was superior to post-training quantization, achieving a **4x reduction in precision with negligible accuracy degradation**.
3. **Knowledge distillation** was most effective when **feature-level knowledge** was transferred, enabling the student model to retain high-level abstractions from the teacher.
4. Combining all three techniques resulted in a **5x reduction in model size, 3x speedup in inference, and only a 1.5% accuracy loss**.

## Conclusion

This research demonstrates that **deep learning models can be significantly optimized** without substantial accuracy trade-offs. By integrating pruning, quantization, and distillation, we can develop **lightweight, efficient models suitable for real-time applications**. Future work includes adapting these techniques to transformer-based architectures and applying them to real-world deployment scenarios.

---

### References
- Han, S., Pool, J., Tran, J., & Dally, W. (2015). "Learning both Weights and Connections for Efficient Neural Networks."
- Hinton, G., Vinyals, O., & Dean, J. (2015). "Distilling the Knowledge in a Neural Network."
- Jacob, B., et al. (2018). "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference."

